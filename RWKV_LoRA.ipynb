{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx7KFfeieD7z"
      },
      "source": [
        "# RWKV LoRA Fine-Tuning\n",
        "\n",
        "[RWKV](https://github.com/BlinkDL/RWKV-LM) is an RNN with transformer-level performance\n",
        "\n",
        "\n",
        "This notebook aims to streamline fine-tuning using [LoRA](https://github.com/Blealtan/RWKV-LM-LoRA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JFIiAsrfvJy"
      },
      "source": [
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_qFjgYmtSfK"
      },
      "outputs": [],
      "source": [
        "#@title Google Drive Options { display-mode: \"form\" }\n",
        "save_models_to_drive = True #@param {type:\"boolean\"}\n",
        "drive_mount = '/content/drive' #@param {type:\"string\"}\n",
        "output_dir = 'rwkv-v4-lora' #@param {type:\"string\"}\n",
        "tuned_model_name = 'tuned' #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "if save_models_to_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount(drive_mount)\n",
        "    output_path = f\"{drive_mount}/MyDrive/{output_dir}\" if save_models_to_drive else f\"/content/{output_dir}\"\n",
        "else:\n",
        "    output_path = \"/content\"\n",
        "\n",
        "tuned_model_dir = f\"{output_path}/{tuned_model_name}\"\n",
        "os.makedirs(tuned_model_dir, exist_ok=True)\n",
        "os.makedirs(f\"{output_path}/base_models/\", exist_ok=True)\n",
        "\n",
        "print(f\"Saving models to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eivKJ6FP1_9z"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4lt0FTegJw9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Blealtan/RWKV-LM-LoRA\n",
        "repo_dir = \"/content/RWKV-LM-LoRA/RWKV-v4neo\"\n",
        "%cd $repo_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDavUrBsgKIV"
      },
      "outputs": [],
      "source": [
        "!pip install transformers pytorch-lightning==1.9 deepspeed wandb ninja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt7y7vR6e6U3"
      },
      "source": [
        "## Load Base Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIgagN-Se3wi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Base Model Options\n",
        "#@markdown Using any of the listed options will download the checkpoint from huggingface\n",
        "base_model_name = \"RWKV-4-Pile-1B5\" #@param [\"RWKV-4-Pile-14B\", \"RWKV-4-Pile-7B\", \"RWKV-4-Pile-3B\", \"RWKV-4-Pile-1B5\", \"RWKV-4-Pile-430M\", \"RWKV-4-Pile-169M\"]\n",
        "\n",
        "if base_model_name == \"RWKV-4-Pile-169M\":\n",
        "    base_model_file = \"RWKV-4-Pile-169M-20220807-8023.pth\"\n",
        "    n_layer = 12\n",
        "    n_embd = 768\n",
        "elif base_model_name == \"RWKV-4-Pile-430M\":\n",
        "    base_model_file = \"RWKV-4-Pile-430M-20220808-8066.pth\"\n",
        "    n_layer = 24\n",
        "    n_embd = 1024\n",
        "elif base_model_name == \"RWKV-4-Pile-1B5\":\n",
        "    base_model_file = \"RWKV-4-Pile-1B5-20220903-8040.pth\"\n",
        "    n_layer = 24\n",
        "    n_embd = 2048\n",
        "elif base_model_name == \"RWKV-4-Pile-3B\":\n",
        "    base_model_file = \"RWKV-4-Pile-3B-20221008-8023.pth\"\n",
        "    n_layer = 32\n",
        "    n_embd = 2560\n",
        "elif base_model_name == \"RWKV-4-Pile-7B\":\n",
        "    base_model_file = \"RWKV-4-Pile-7B-20221115-8047.pth\"\n",
        "    n_layer = 32\n",
        "    n_embd = 4096\n",
        "elif base_model_name == \"RWKV-4-Pile-14B\":\n",
        "    base_model_file = \"RWKV-4-Pile-14B-20230213-8019.pth\"\n",
        "    n_layer = 40\n",
        "    n_embd = 5120\n",
        "\n",
        "base_model_url = f\"https://huggingface.co/BlinkDL/{base_model_name.lower()}/resolve/main/{base_model_file}\"\n",
        "base_model_path = f\"{output_path}/base_models/{base_model_file}\"\n",
        "\n",
        "if save_models_to_drive and not os.path.exists(base_model_path):\n",
        "    import urllib.request\n",
        "    print(f\"Downloading {base_model_name} this may take a while\")\n",
        "    urllib.request.urlretrieve(base_model_url, base_model_path)\n",
        "\n",
        "print(f\"Using {base_model_path} as base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCOPnLelfJgP"
      },
      "source": [
        "## Generate Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW5OmlXmvaIU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Training Data Options\n",
        "#@markdown `input_file` should be the path to a single file that contains the text you want to fine-tune with.\n",
        "#@markdown Either upload a file to this notebook instance or reference a file in your Google drive.\n",
        "\n",
        "import numpy as np\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=f'{repo_dir}/20B_tokenizer.json')\n",
        "\n",
        "input_file = \"/content/drive/MyDrive/training.txt\" #@param {type:\"string\"}\n",
        "output_file = 'train.npy'\n",
        "\n",
        "print(f'Tokenizing {input_file} (VERY slow. please wait)')\n",
        "\n",
        "data_raw = open(input_file, encoding=\"utf-8\").read()\n",
        "print(f'Raw length = {len(data_raw)}')\n",
        "\n",
        "data_code = tokenizer.encode(data_raw)\n",
        "print(f'Tokenized length = {len(data_code)}')\n",
        "\n",
        "out = np.array(data_code, dtype='uint16')\n",
        "np.save(output_file, out, allow_pickle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4lz-3maeIwY"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Begin Training with these Options { display-mode: \"form\" }\n",
        "n_epoch = 100 #@param {type:\"integer\"}\n",
        "epoch_save_frequency = 5 #@param {type:\"integer\"}\n",
        "batch_size =  1 #@param {type:\"integer\"} \n",
        "ctx_len = 1024 #@param {type:\"integer\"}\n",
        "vocab_size = 50277 #@param {type:\"integer\"}\n",
        "\n",
        "!python3 train.py \\\n",
        "  --load_model $base_model_path \\\n",
        "  --proj_dir $tuned_model_dir \\\n",
        "  --data_file \"train.npy\" \\\n",
        "  --data_type \"numpy\" \\\n",
        "  --vocab_size $vocab_size \\\n",
        "  --ctx_len $ctx_len \\\n",
        "  --epoch_save $epoch_save_frequency \\\n",
        "  --epoch_count $n_epoch \\\n",
        "  --n_layer $n_layer \\\n",
        "  --n_embd $n_embd \\\n",
        "  --epoch_steps 1000 --epoch_begin 0  --micro_bsz $batch_size --pre_ffn 0 --head_qk 0 --lr_init 1e-5 --lr_final 1e-5 --warmup_steps 0 --beta1 0.9 --beta2 0.999 --adam_eps 1e-8 --accelerator gpu --devices 1 --precision bf16 --strategy deepspeed_stage_2 --grad_cp 0 \\\n",
        "  --lora --lora_r 8 --lora_alpha 32 --lora_dropout 0.01"
      ],
      "metadata": {
        "id": "eIHp58bSJ-Jw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1ITfSRrRpVO9Xz-6MYDWVHHWykp02L4qi",
      "authorship_tag": "ABX9TyOyKNCZQd1UVZVqguVnSDML",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}