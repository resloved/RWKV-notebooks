{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/resloved/RWKV-notebooks/blob/master/RWKV_v3_RNN_Pile_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx7KFfeieD7z"
      },
      "source": [
        "# RWKV-v3-RNN-Pile Fine-Tuning\n",
        "\n",
        "[RWKV](https://github.com/BlinkDL/RWKV-LM) is an RNN with transformer-level performance\n",
        "\n",
        "\n",
        "This notebook aims to streamline fine-tuning [RWKV-v2-RNN-Pile](https://github.com/BlinkDL/RWKV-v2-RNN-Pile) as detailed [here](https://github.com/BlinkDL/RWKV-v2-RNN-Pile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JFIiAsrfvJy"
      },
      "source": [
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_qFjgYmtSfK"
      },
      "outputs": [],
      "source": [
        "#@title Google Drive Options { display-mode: \"form\" }\n",
        "save_models_to_drive = True #@param {type:\"boolean\"}\n",
        "drive_mount = '/content/drive' #@param {type:\"string\"}\n",
        "output_dir = 'rwkv-v3-rnn-pile-tuning' #@param {type:\"string\"}\n",
        "tuned_model_name = 'tuned' #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(drive_mount, force_remount=True)\n",
        "\n",
        "output_path = f\"{drive_mount}/MyDrive/{output_dir}\" if save_models_to_drive else f\"/content/{output_dir}\"\n",
        "os.makedirs(f\"{output_path}/{tuned_model_name}\", exist_ok=True)\n",
        "os.makedirs(f\"{output_path}/base_models/\", exist_ok=True)\n",
        "\n",
        "print(f\"Saving models to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eivKJ6FP1_9z"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6_RM1m292ZW"
      },
      "outputs": [],
      "source": [
        "if save_models_to_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "uXcjyRatABi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4lt0FTegJw9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/blinkdl/RWKV-v2-RNN-Pile\n",
        "repo_dir = \"/content/RWKV-v2-RNN-Pile/RWKV-v3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDavUrBsgKIV"
      },
      "outputs": [],
      "source": [
        "!pip install transformers wandb ninja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt7y7vR6e6U3"
      },
      "source": [
        "## Load Base Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIgagN-Se3wi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Base Model Options\n",
        "#@markdown Using any of the listed options will download the checkpoint from huggingface\n",
        "\n",
        "base_model_name = \"rwkv-3-pile-430m\" #@param [\"rwkv-3-pile-1b5\", \"rwkv-3-pile-430m\", \"rwkv-3-pile-169m\"]\n",
        "\n",
        "!git lfs clone https://huggingface.co/BlinkDL/$base_model_name\n",
        "\n",
        "from glob import glob\n",
        "base_model_path = glob(f\"{base_model_name}/RWKV*.pth\")[0]\n",
        "\n",
        "print(f\"Using {base_model_path} as base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCOPnLelfJgP"
      },
      "source": [
        "## Generate Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wW5OmlXmvaIU"
      },
      "outputs": [],
      "source": [
        "#@title Training Data Options\n",
        "#@markdown `input_file` should be the path to a single file that contains the text you want to fine-tune with.\n",
        "#@markdown Either upload a file to this notebook instance or reference a file in your Google drive.\n",
        "\n",
        "import numpy as np\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file=f'{repo_dir}/20B_tokenizer.json')\n",
        "\n",
        "input_file = \"/content/drive/MyDrive/training.txt\" #@param {type:\"string\"}\n",
        "output_file = 'train.npy'\n",
        "\n",
        "print(f'Tokenizing {input_file} (VERY slow. please wait)')\n",
        "\n",
        "data_raw = open(input_file, encoding=\"utf-8\").read()\n",
        "print(f'Raw length = {len(data_raw)}')\n",
        "\n",
        "data_code = tokenizer.encode(data_raw)\n",
        "print(f'Tokenized length = {len(data_code)}')\n",
        "\n",
        "out = np.array(data_code, dtype='uint16')\n",
        "np.save(output_file, out, allow_pickle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4lz-3maeIwY"
      },
      "source": [
        "## Fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuCw5_ASwMud"
      },
      "outputs": [],
      "source": [
        "#@title Fine-tuning Options { display-mode: \"form\" }\n",
        "\n",
        "#@markdown By default the fine tuning is handled in GPT mode as it trains much faster,\n",
        "#@markdown however it uses much more VRAM.\n",
        "#@markdown\n",
        "#@markdown The suggested settings for training the 430M paramater model are normally a `ctx_len` of 768, a `batch_size` of 8, and `B_GROUP_FORWARD` being 8.\n",
        "#@markdown However with the limited VRAM you get with a P100 I've found `336`/`4`/`4` doable. \n",
        "#@markdown\n",
        "#@markdown As always your mileage may vary, fiddle with the numbers yourself.\n",
        "#@markdown\n",
        "#@markdown ---\n",
        "#@markdown \n",
        "\n",
        "#@markdown Enable `use_wandb` if you want to track your training run via [Weights & Biases](https://wandb.ai)\n",
        "use_wandb = True #@param {type:\"boolean\"}\n",
        "#@markdown Epochs in this context are really \"mini-epochs\" that are quite short. They have a fixed length of `ctx_len * epoch_length_fixed` tokens\n",
        "n_epoch = 100#@param {type:\"integer\"}\n",
        "epoch_save_frequency = 4 #@param {type:\"integer\"}\n",
        "ctx_len = 384 #@param {type:\"integer\"}\n",
        "#@markdown If finetuning OOMs, consider lowering the batch size before lowering `ctx_len`\n",
        "batch_size =  4#@param {type:\"integer\"} \n",
        "#@markdown `batch_size` must be divisible by both `B_GROUP_FORWARD` and `B_GROUP_BACKWARD`\n",
        "B_GROUP_FORWARD =  4#@param {type:\"integer\"}\n",
        "B_GROUP_BACKWARD =  2#@param {type:\"integer\"}\n",
        "\n",
        "import logging\n",
        "import datetime\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import json\n",
        "import sys\n",
        "\n",
        "import wandb\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.cpp_extension import load\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "n_layer = 24\n",
        "n_embd = 2048\n",
        "\n",
        "if \"430\" in base_model_name:\n",
        "    n_layer = 24\n",
        "    n_embd = 1024\n",
        "elif \"169\" in base_model_name:\n",
        "    n_layer = 12\n",
        "    n_embd = 768\n",
        "\n",
        "vocab_size = 50277\n",
        "\n",
        "model_type = 'RWKV'\n",
        "datafile = 'train.npy'\n",
        "\n",
        "#@markdown If your training data uses something similar to what is already in [The Pile](https://pile.eleuther.ai/)\n",
        "#@markdown consider setting `lr_init` to `1e-5` otherwise `2e-5`\n",
        "lr_init = 1e-5#@param {type:\"number\"}\n",
        "lr_final = 1e-5#@param {type:\"number\"}\n",
        "\n",
        "T_MAX = ctx_len\n",
        "epoch_length_fixed = 10000\n",
        "\n",
        "epoch_save_path = f\"{output_path}/\"\n",
        "\n",
        "grad_norm_clip = 1.0\n",
        "warmup_tokens = 0\n",
        "\n",
        "eps=1e-8\n",
        "\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "num_workers = 0\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvV3C1wEgwJJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title RWKV_GPT\n",
        "timex_cuda = load(name=\"timex\", sources=[f\"{repo_dir}/cuda/timex_op.cpp\", f\"{repo_dir}/cuda/timex_cuda.cu\"],\n",
        "                  verbose=True, extra_cuda_cflags=['--use_fast_math', '--extra-device-vectorization', f'-DTmax={T_MAX}', f'-DBF={B_GROUP_FORWARD}', f'-DBB={B_GROUP_BACKWARD}'])\n",
        "\n",
        "\n",
        "class TimeX(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, w, k, B, C, T, eps):\n",
        "        ctx.B = B\n",
        "        ctx.C = C\n",
        "        ctx.T = T\n",
        "        assert ctx.T % 4 == 0 and ctx.T <= T_MAX and ctx.B % B_GROUP_FORWARD == 0 and ctx.B % B_GROUP_BACKWARD == 0\n",
        "        w = w.contiguous()\n",
        "        k = k.contiguous()\n",
        "        ctx.save_for_backward(w, k)\n",
        "        wk = torch.empty((B, C, T), device='cuda',\n",
        "                         memory_format=torch.contiguous_format)\n",
        "        timex_cuda.forward(w, k, wk, eps, B, C, T)\n",
        "        return wk\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, gwk):\n",
        "        assert ctx.T % 4 == 0 and ctx.T <= T_MAX and ctx.B % B_GROUP_FORWARD == 0 and ctx.B % B_GROUP_BACKWARD == 0\n",
        "        w, k = ctx.saved_tensors\n",
        "        gw = torch.empty((ctx.B, ctx.C, ctx.T), device='cuda',\n",
        "                         memory_format=torch.contiguous_format)\n",
        "        gk = torch.empty((ctx.B, ctx.C, ctx.T), device='cuda',\n",
        "                         memory_format=torch.contiguous_format)\n",
        "        timex_cuda.backward(w, k, gwk.contiguous(), gw,\n",
        "                            gk, ctx.B, ctx.C, ctx.T)\n",
        "        return (gw.sum(dim=0), gk, None, None, None, None)\n",
        "\n",
        "########################################################################################################\n",
        "# RWKV: RWKV Time-mix + RWKV Channel-mix\n",
        "########################################################################################################\n",
        "\n",
        "RWKV_K_CLAMP = 60  # e^60 = 1e26\n",
        "RWKV_K_EPS = 1e-9\n",
        "RWKV_HEAD_QK_DIM = 256\n",
        "\n",
        "class RWKV_TimeMix(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.layer_id = layer_id\n",
        "        self.ctx_len = config.ctx_len\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        attn_sz = config.n_embd\n",
        "\n",
        "        self.time_decay = nn.Parameter(torch.ones(attn_sz, 1))\n",
        "        self.time_curve = torch.tensor(\n",
        "            [-(config.ctx_len - 2 - i) for i in range(config.ctx_len-1)]).unsqueeze(0)\n",
        "        self.time_curve = self.time_curve.to('cuda')\n",
        "        self.time_first = nn.Parameter(torch.ones(attn_sz, 1) * math.log(0.3))\n",
        "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
        "        with torch.no_grad():\n",
        "            ww = torch.ones(1, 1, config.n_embd)\n",
        "            for i in range(config.n_embd // 2):\n",
        "                ww[0, 0, i] = 0\n",
        "        self.time_mix_k = nn.Parameter(ww)\n",
        "        self.time_mix_v = nn.Parameter(ww)\n",
        "        self.time_mix_r = nn.Parameter(ww)\n",
        "\n",
        "        self.key = nn.Linear(config.n_embd, attn_sz, bias=False)\n",
        "        self.value = nn.Linear(config.n_embd, attn_sz, bias=False)\n",
        "        self.receptance = nn.Linear(config.n_embd, attn_sz, bias=False)\n",
        "\n",
        "        self.output = nn.Linear(attn_sz, config.n_embd, bias=False)\n",
        "\n",
        "        self.key.scale_init = 0\n",
        "        self.receptance.scale_init = 0\n",
        "        self.output.scale_init = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        assert T == self.ctx_len\n",
        "\n",
        "        xx = self.time_shift(x)\n",
        "        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n",
        "        xv = x * self.time_mix_v + xx * (1 - self.time_mix_v)\n",
        "        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n",
        "\n",
        "        k = self.key(xk).transpose(-1, -2)\n",
        "        v = self.value(xv).transpose(-1, -2)\n",
        "        r = self.receptance(xr)\n",
        "\n",
        "        # RWKV_K_CLAMP can be removed if the CUDA kernel substracts the correct k_max for each k (I will do this later)\n",
        "        k = torch.clamp(k, max=RWKV_K_CLAMP)\n",
        "        k = torch.exp(k)\n",
        "        kv = k * v\n",
        "\n",
        "        self.time_w = torch.cat(\n",
        "            [torch.exp(self.time_decay) * self.time_curve, self.time_first], dim=-1)\n",
        "        w = torch.exp(self.time_w)\n",
        "\n",
        "        wkv = TimeX.apply(w, kv, B, C, T, 0)\n",
        "        # RWKV_K_EPS can be removed if the CUDA kernel sets 0/0 = 0 (I will do this later)\n",
        "        wk = TimeX.apply(w, k, B, C, T, RWKV_K_EPS)\n",
        "\n",
        "        rwkv = torch.sigmoid(r) * (wkv / wk).transpose(-1, -2)\n",
        "        rwkv = self.output(rwkv)\n",
        "        return rwkv\n",
        "\n",
        "\n",
        "class RWKV_ChannelMix(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.layer_id = layer_id\n",
        "\n",
        "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
        "\n",
        "        with torch.no_grad():  # init to \"shift half of the channels\"\n",
        "            x = torch.ones(1, 1, config.n_embd)\n",
        "            for i in range(config.n_embd // 2):\n",
        "                x[0, 0, i] = 0\n",
        "        self.time_mix_k = nn.Parameter(x)\n",
        "        self.time_mix_r = nn.Parameter(x)\n",
        "\n",
        "        hidden_sz = 4 * config.n_embd\n",
        "        self.key = nn.Linear(config.n_embd, hidden_sz, bias=False)\n",
        "        self.receptance = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "        self.value = nn.Linear(hidden_sz, config.n_embd, bias=False)\n",
        "\n",
        "        self.value.scale_init = 0\n",
        "        self.receptance.scale_init = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        xx = self.time_shift(x)\n",
        "        xk = x * self.time_mix_k + xx * (1 - self.time_mix_k)\n",
        "        xr = x * self.time_mix_r + xx * (1 - self.time_mix_r)\n",
        "\n",
        "        k = self.key(xk)\n",
        "        k = torch.square(torch.relu(k))\n",
        "        kv = self.value(k)\n",
        "        \n",
        "        rkv = torch.sigmoid(self.receptance(xr)) * kv\n",
        "        return rkv\n",
        "\n",
        "########################################################################################################\n",
        "# The GPT Model with our blocks\n",
        "########################################################################################################\n",
        "\n",
        "\n",
        "class GPTConfig:\n",
        "    def __init__(self, vocab_size, ctx_len, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.ctx_len = ctx_len\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_id = layer_id\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        if self.layer_id == 0:\n",
        "            self.ln0 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        self.att = RWKV_TimeMix(config, layer_id)\n",
        "        self.ffn = RWKV_ChannelMix(config, layer_id)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        if self.layer_id == 0:\n",
        "            x = self.ln0(x)\n",
        "        x = x + self.att(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.step = 0\n",
        "        self.config = config\n",
        "\n",
        "        self.emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[Block(config, i)\n",
        "                                    for i in range(config.n_layer)])\n",
        "\n",
        "        self.ln_out = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.ctx_len = config.ctx_len\n",
        "\n",
        "        # RWKV_Init(self, config)\n",
        "\n",
        "        logger.info(\"number of parameters: %e\", sum(p.numel()\n",
        "                    for p in self.parameters()))\n",
        "\n",
        "    def get_ctx_len(self):\n",
        "        return self.ctx_len\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.01)\n",
        "        if isinstance(module, (nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=1e-5)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "\n",
        "        for mn, m in self.named_modules():  # here we disable weight_decay\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n",
        "                no_decay.add(fpn)\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(\n",
        "            inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "            % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn]\n",
        "                        for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "\n",
        "        optimizer = torch.optim.Adam(\n",
        "            optim_groups, lr=train_config.learning_rate, betas=train_config.betas, eps=train_config.eps)\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        self.step += 1\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.ctx_len, \"Cannot forward, because len(input) > model ctx_len.\"\n",
        "        x = self.emb(idx)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_out(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(x.view(-1, x.size(-1)), targets.view(-1))\n",
        "\n",
        "        return x, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHIJp8vmmWX8"
      },
      "outputs": [],
      "source": [
        "#@title Trainer { display-mode: \"form\" }\n",
        "\n",
        "log_file = open(\"mylog.txt\", \"a\")\n",
        "\n",
        "class TrainerConfig:\n",
        "    max_epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 4e-4\n",
        "    betas = (0.9, 0.99)\n",
        "    eps = 1e-8\n",
        "    grad_norm_clip = 1.0\n",
        "    lr_decay = True  # linear warmup followed by cosine decay\n",
        "    warmup_tokens = 0\n",
        "    final_tokens = 0\n",
        "    epoch_save_frequency = 0\n",
        "    epoch_save_path = 'trained-'\n",
        "    num_workers = 0  # for DataLoader\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, train_dataset, test_dataset, config):\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.config = config\n",
        "        self.avg_loss = -1\n",
        "        self.steps = 0\n",
        "\n",
        "        if use_wandb and 'wandb' in sys.modules:\n",
        "            cfg = model.config\n",
        "            for k in config.__dict__:\n",
        "                setattr(cfg, k, config.__dict__[k])  # combine cfg\n",
        "            wandb.init(project=\"RWKV-LM\", name=f\"{tuned_model_name}-{self.get_run_name()}\", config=cfg, save_code=False)\n",
        "\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():  # take over whatever gpus are on the system\n",
        "            self.device = torch.cuda.current_device()\n",
        "\n",
        "    def get_run_name(self):\n",
        "        raw_model = self.model.module if hasattr(\n",
        "            self.model, \"module\") else self.model\n",
        "        cfg = raw_model.config\n",
        "        run_name = str(cfg.vocab_size) + '-' + str(cfg.ctx_len) + '-' + str(cfg.n_layer) + '-' + str(cfg.n_embd)\n",
        "        return run_name\n",
        "\n",
        "    def train(self):\n",
        "        model, config = self.model, self.config\n",
        "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
        "        optimizer = raw_model.configure_optimizers(config)\n",
        "\n",
        "        def run_epoch(split):\n",
        "            is_train = split == 'train'\n",
        "            model.train(is_train)\n",
        "            data = self.train_dataset if is_train else self.test_dataset\n",
        "\n",
        "            if config.num_workers > 0:\n",
        "                loader = DataLoader(data, shuffle=False, pin_memory=True,\n",
        "                                    batch_size=config.batch_size,\n",
        "                                    num_workers=config.num_workers)\n",
        "            else:\n",
        "                loader = DataLoader(data, shuffle=False,\n",
        "                                    batch_size=config.batch_size,\n",
        "                                    num_workers=config.num_workers)\n",
        "\n",
        "            pbar = tqdm(enumerate(loader), total=len(\n",
        "                loader), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') if is_train else enumerate(loader)\n",
        "\n",
        "            for it, (x, y) in pbar:\n",
        "                x = x.to(self.device)  # place data on the correct device\n",
        "                y = y.to(self.device)\n",
        "\n",
        "                with torch.set_grad_enabled(is_train):\n",
        "                    _, loss = model(x, y)  # forward the model\n",
        "\n",
        "                if is_train:  # backprop and update the parameters\n",
        "                    model.zero_grad()\n",
        "                    loss.backward()\n",
        "\n",
        "                    if config.grad_norm_clip > 0:\n",
        "                        torch.nn.utils.clip_grad_norm_(\n",
        "                            model.parameters(), config.grad_norm_clip)\n",
        "\n",
        "                    optimizer.step()\n",
        "\n",
        "                    if config.lr_decay:  # decay the learning rate based on our progress\n",
        "                        # number of tokens processed this step (i.e. label is not -100)\n",
        "                        self.tokens += (y >= 0).sum()\n",
        "                        lr_final_factor = config.lr_final / config.learning_rate\n",
        "                        if self.tokens < config.warmup_tokens:\n",
        "                            # linear warmup\n",
        "                            lr_mult = lr_final_factor + \\\n",
        "                                (1 - lr_final_factor) * float(self.tokens) / \\\n",
        "                                float(config.warmup_tokens)\n",
        "                            progress = 0\n",
        "                        else:\n",
        "                            # exponential learning rate decay\n",
        "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                            if progress >= 1:\n",
        "                                lr_mult = lr_final_factor\n",
        "                            else:\n",
        "                                lr_mult = math.exp(math.log(lr_final_factor) * pow(progress, 1))\n",
        "                        lr = config.learning_rate * lr_mult\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                    else:\n",
        "                        lr = config.learning_rate\n",
        "\n",
        "                    now_loss = loss.item()  # report progress\n",
        "                    self.lr = lr\n",
        "\n",
        "                    if use_wandb and 'wandb' in sys.modules:\n",
        "                        wandb.log({\"loss\": now_loss},\n",
        "                                  step=self.steps * self.config.batch_size)\n",
        "                    self.steps += 1\n",
        "\n",
        "                    if self.avg_loss < 0:\n",
        "                        self.avg_loss = now_loss\n",
        "                    else:\n",
        "                        factor = 1 / (it + 1)\n",
        "                        self.avg_loss = self.avg_loss * \\\n",
        "                            (1.0 - factor) + now_loss * factor\n",
        "                    pbar.set_description(\n",
        "                        f\"mini-epoch {epoch+1} prog {progress*100.0:.2f}% iter {it}: ppl {math.exp(self.avg_loss):.2f} loss {self.avg_loss:.4f} lr {lr:e}\")\n",
        "\n",
        "        self.tokens = 0  # counter used for learning rate decay\n",
        "        for epoch in range(config.max_epochs):\n",
        "\n",
        "            run_epoch('train')\n",
        "\n",
        "            log_file.write(\n",
        "                f'{epoch+1} {self.avg_loss:.6f} {math.exp(self.avg_loss):.4f} {self.lr:.8f} {datetime.datetime.now()} \\n')\n",
        "            log_file.flush()\n",
        "\n",
        "            if (self.config.epoch_save_frequency > 0 and epoch % self.config.epoch_save_frequency == 0) or (epoch == config.max_epochs - 1):\n",
        "                # DataParallel wrappers keep raw model object in .module\n",
        "                raw_model = self.model.module if hasattr(\n",
        "                    self.model, \"module\") else self.model\n",
        "                torch.save(raw_model.state_dict(), f\"{output_path}/{tuned_model_name}/{tuned_model_name}-{base_model_name}-{trainer.get_run_name()}-{epoch}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyCW0Wp4gk4p",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Dataset\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, data, vocab_size, ctx_len, epoch_length_fixed):\n",
        "        data_size, vocab_size = len(data), vocab_size\n",
        "        print('data has %d tokens, %d unique.' % (data_size, vocab_size))\n",
        "        self.ctx_len = ctx_len\n",
        "        self.epoch_length_fixed = epoch_length_fixed\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.epoch_length_fixed\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # cheat: pick a random spot in dataset\n",
        "        i = np.random.randint(0, len(self.data) - (self.ctx_len + 1))\n",
        "        dix = self.data[i:i+self.ctx_len+1]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long,\n",
        "                         device=torch.device('cuda'))\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long,\n",
        "                         device=torch.device('cuda'))\n",
        "        return x, y\n",
        "\n",
        "print('loading data... ' + datafile)\n",
        "train_dataset = Dataset(np.load(datafile).astype('int'), vocab_size, ctx_len, epoch_length_fixed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5nfvk9lkHo3"
      },
      "outputs": [],
      "source": [
        "#@title Start fine-tuning { display-mode: \"form\" }\n",
        "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n",
        "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "                    datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO,)\n",
        "\n",
        "model = GPT(GPTConfig(train_dataset.vocab_size, train_dataset.ctx_len, model_type=model_type,\n",
        "                      n_layer=n_layer, n_embd=n_embd)).cuda()\n",
        "\n",
        "print('loading ' + base_model_path)\n",
        "m2 = torch.load(base_model_path)\n",
        "model.load_state_dict(m2)\n",
        "\n",
        "print('model', model_type, 'epoch', n_epoch, 'batchsz', batch_size, 'betas',\n",
        "      betas, 'eps', eps, 'ctx', ctx_len, 'layer', n_layer, 'embd', n_embd, )\n",
        "tconf = TrainerConfig(model_type=model_type, max_epochs=n_epoch, batch_size=batch_size,\n",
        "                      learning_rate=lr_init, lr_decay=True, lr_final=lr_final, betas=betas, eps=eps, grad_norm_clip=grad_norm_clip,\n",
        "                      warmup_tokens=warmup_tokens, final_tokens=n_epoch*len(train_dataset)*ctx_len, num_workers=num_workers, epoch_save_frequency=epoch_save_frequency, epoch_save_path=epoch_save_path)\n",
        "trainer = Trainer(model, train_dataset, None, tconf)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "tuned_model_path = f\"{output_path}/{tuned_model_name}/{tuned_model_name}-{base_model_name}-{trainer.get_run_name()}.pth\"\n",
        "torch.save(model.state_dict(), tuned_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tiZvudz5jLoD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RWKV-v3-RNN-Pile Fine-Tuning",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1ITfSRrRpVO9Xz-6MYDWVHHWykp02L4qi",
      "authorship_tag": "ABX9TyMoOdYuztgjgteIpoufKooZ",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}